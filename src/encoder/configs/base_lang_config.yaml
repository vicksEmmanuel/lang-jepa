data:
  # Dataset configuration
  train_file: "sample-10BT"  # FineWeb-Edu dataset file
  batch_size: 32
  num_workers: 4
  tokenizer_path: "roberta-base"  # Base tokenizer to use
#  tokenizer_path: "bert-base-uncased"
#  tokenizer_path: "gpt2"
  limit: 100  # Number of training samples to use
  min_length: 100  # Minimum text length to consider
  min_sentences: 2  # Minimum number of sentences (needed for context/target split)

model:
  max_length: 128  # Should be smaller, around 128-256
  pred_dim: 384
  embed_dim: 768   # Standard size for base models
  num_layers: 12
  num_heads: 12
  mlp_ratio: 4.0
  dropout: 0.1

optimization:
  # Training parameters
  epochs: 5
  lr: 0.001              # Peak learning rate
  warmup: 1              # Warmup epochs
  weight_decay: 0.04     # Initial weight decay
  final_weight_decay: 0.4  # Final weight decay
  final_lr: 0.000001     # Final learning rate after decay

logging:
  # Logging configuration
  log_dir: "logs/lang_jepa"  # Directory for saving logs and checkpoints
  log_freq: 50               # How often to log training metrics (iterations)
  checkpoint_freq: 1         # How often to save checkpoints (epochs)
  num_examples: 3            # Number of examples to monitor during training
  log_to_wandb: true        # Whether to log to Weights & Biases

meta:
  # Mixed precision and checkpointing
  use_bfloat16: false                 # Whether to use bfloat16 mixed precision
  load_checkpoint: false              # Whether to load from checkpoint
  checkpoint_path: null               # Path to checkpoint if loading